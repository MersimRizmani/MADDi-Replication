{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MersimRizmani/MADDi-Replication/blob/main/CS_598_Team_20_Project_Draft_Notebook_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CS 598 Project: Multimodal Attention for Alzheimer's Disease Classification - Replication of MADDi Framework**\n",
        "\n",
        "### Project GitHub Repository: https://github.com/MersimRizmani/MADDi-Replication.git\n",
        "### Team Members\n",
        "| Name | NetId |\n",
        "|----------|----------|\n",
        "| Mersim Rizmani | mrizma2 |\n",
        "| Abhilash Raghuram | araghu9 |\n",
        "| Jacob Men | men5 |\n",
        "\n"
      ],
      "metadata": {
        "id": "bOjUVNq-gnw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wujMY5ZAT2Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statement on the Use of Existing Code\n",
        "For our project, we will be utilizing the existing code in an effort to replicate the original study. Specifically, we will attempt to replicate the MADDi framework that was developed in the original study. The original code can be found here: https://github.com/rsinghlab/MADDi/tree/main\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wcFnyHYATz9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citation of Original Paper\n",
        "\n",
        "Below are the references to the original research paper published to JAMIA, and the code repository:\n",
        "\n",
        "Michal Golovanevsky, Carsten Eickhoff, Ritambhara Singh, Multimodal attention-based deep learning for Alzheimer’s disease diagnosis, Journal of the American Medical Informatics Association, Volume 29, Issue 12, December 2022, Pages 2014–2022, https://doi.org/10.1093/jamia/ocac168\n",
        "\n",
        "GitHub Repository: https://github.com/rsinghlab/MADDi/tree/main\n",
        "\n",
        "\\\n",
        "\n",
        "## Background\n",
        "\n",
        "The general problem that this project focuses on is accurately diagnosing Alzheimer’s disease in susceptible patients. The objective of the original study was to develop a multimodal deep learning framework that could aid medical professionals in accomplishing this task and making Alzheimer’s diagnosis easier and more accurate.\n",
        "\\\n",
        "\\\n",
        "Why is it important to improve the process of Alzheimer’s disease diagnosis utilizing deep learning models? Well, according to statistics from Alzheimer’s News Today, Alzheimer’s disease is the most common neurodegenerative disorder affecting approximately 5.5 million people in the United States, and around 44 million people worldwide. On top of that, research has also shown that less than half of Alzheimer’s patients are diagnosed accurately for pathology and disease progression based on clinical symptoms alone. This statistic alone highlights the urgent need for advancement in Alzheimer’s disease diagnosis, and that is why deep learning researchers are working to create frameworks to solve this problem.\n",
        "\\\n",
        "\\\n",
        "The difficultly of this problem, as mentioned in the original research, lies in cross-modal interactions. Until this research had been conducted, several previous deep learning-based studies lacked focus on cross-modal interactions and simply focused on conjoined features extracted from disjoint modalities. This model that we will be repliciating in this project, MADDi, will seek to response and fill the gaps of previous multimodal studies.\n",
        "\n",
        "\\\n",
        "\n",
        "## Original Paper Explanation\n",
        "\n",
        "The approach taken by this research paper was centered around a multimodal deep learning framework called MADDi, short for Multimodal Alzheimer’s Disease Diagnosis framework. It utilizes a cross-modal attention scheme to integrate imaging data (i.e. MRI data), genetic data (i.e. SNPs), and structured clinical data to classify patients and attempt to label (diagnose) them. The entire pipeline involves clinical, image, and genetic data preprocessing, building the multimodal framework, neural network attention, unified hyperparameter tuning, and model evaluation.\n",
        "\\\n",
        "\\\n",
        "In our project, we seek to replicate this study to the best of our ability by attempting to rebuild MADDi in our own notebook. From this process, our goal will be to fully understand and learn how real medical data, such as that from ADNI, can be paired with a functioning deep learning framework to greatly improve the accuracy of Alzheimer’s disease diagnosis.\n",
        "\\\n",
        "\\\n",
        "The researchers found that MADDi was superior in performance to existing multimodal studies and was proven to be consistently high in accuracy, achieving figures as high as 97% for Alzheimer's classification. This study contributes heavily to the research on Alzheimer's Disease, and the medical community as a whole, as it provides a window of potential into what's possible with automated and accuated deep learning models for disease diagnosis.  \n"
      ],
      "metadata": {
        "id": "KKoleuHytoJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility\n",
        "\n",
        "### Hypothesis\n",
        "\n",
        "The hypothesis we want to test according to this research paper is whether a novel multimodal deep learning framework, in this case MADDi, can accurately aid medical professionals in diagnosing Alzheimer’s disease. Specifically, we hypothesize that integrating multiple modalities (i.e. imaging, genetic, and clinical data) using a cross-modal attention scheme will lead to improved accuracy in Alzheimer’s diagnosis, compared to unimodal approaches.\n",
        "\n",
        "\\\n",
        "### Ablations\n",
        "This project will also involve the replication of two ablations, or experimental variations, presented in the original research paper:\n",
        "1. Attention Mechanisms: Ablations will be conducted by toggling the presence of attention based on four criteria: self-attention and cross-modal attention, just cross-modal attention, and no attention. The variations will be generalized attention, self-attention, and cross-modal attention.\n",
        "2. Unified Hyperparameter Tuning Scheme: Ablations will vary by toggling the methods for optimizing hyperparameters in the model. The scheme will allow for generalizing the model and tuning it for specific scenarios without manual intervention.\n",
        "\n",
        "\n",
        "![maddi.jpeg](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/jamia/29/12/10.1093_jamia_ocac168/1/ocac168f2.jpeg?Expires=1715219500&Signature=JlUSynqR~d5I6VW7O9wXDescNbaAjm02F3mL5TbpHcm0-lfxXQaw2F4sk24sLg0c2K6AqGrYMblT~VI1VZ5TNyf3HVpqeUakuz~EoQpCWrmhKhoXq277Zj8gdhZ5yP5kXyzewf9BKMakGRBTJoW7Trs9jayoq4CrofKZGd03~Vd4tF1kZu27FF~N8kiidoHH5zW6~fQsdokE672zPsdfdERefEhpRmVIBvIVFPOaKGJlqFzVxKkY8vtYwf-pmWXyyS5d7X4Os~3X3Hq~X6JiajD-86QMXDtsdPQ-OVj4hmvDKRnJ6gpoAzXrVASwnnXx54tS21~D9552D5JBxZdRSw__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)"
      ],
      "metadata": {
        "id": "4Qs8IN87SWtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Source**\n",
        "\n",
        "The data that will be processed and analyzed in this project and study replication has to be requested directly from the Alzheimer’s Disease Neuroimaging Initiative (ADNI), https://adni.loni.usc.edu/data-samples/access-data/.\n",
        "\n",
        "All ADNI data are shared without embargo through the LONI Image and Data Archive (IDA), https://ida.loni.usc.edu/login.jsp?project=ADNI, a secure research data repository. Access is contingent on adherence to the ADNI Data Use Agreement and the publications’ policies.\n",
        "\n",
        "Once access is granted from the LONI Image and Data Archive, which from our experience has a timeline of approximately two weeks, then we look for the necessary files in the data archive to download. In our case, we’d be looking for clinical, genetic, and imaging data from three studies: ADNI1, ADNI2, and ADNI GO."
      ],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Description**\n",
        "\n",
        "**Clinical**: Neurological exams, cognitive assessments, and patient demographics from 2384 patients.\n",
        "\n",
        "- Type: Quanititative, categorical, binary\n",
        "- Features: 29\n",
        "\n",
        "**Genetic**: Genome sequencing data from 805 ADNI participants. Each subject had about 3 million SNPs in the raw VCF generated.\n",
        "\n",
        "- Type: VCF (variant call files)\n",
        "\n",
        "**Imaging**: Cross-sectional MRI data corresponding to first baseline screenings from ADNI1 (551 patients)."
      ],
      "metadata": {
        "id": "7PVvviKfuGV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Statistics**\n",
        "\n",
        "#### Data Storage and Size\n",
        "\n",
        "A key challenge for this project was the sheer amount of data we were handling. It took several days to transfer data from ADNI to our shared Google Drive where we housed all the data. The specific size of our dataset were rougly equivalent to:\n",
        "\n",
        "- Genetic Data (VCF files): ~45 GB\n",
        "- Imaging Data (images and metadata): ~10 GB\n",
        "- Clinical Data: ~1 GB\n",
        "\n",
        "#### Basic Statistics Obtained from Data Preprocessing\n",
        "\n",
        "The following statistics were obtained from clinical data preprocessing in the decision_making.ipynb file:\n",
        "\n",
        "Number of diagnosed patients: 3025\n",
        "\n",
        "- Number of Normal patients: 1122\n",
        "- Number of Mild Cognitive Impairment patients: 1007\n",
        "- Number of Alzheimer's Disease patients: 896\n",
        "\n",
        "and the following statistics are from the original research paper, showing the number of participants in each modality and further separating the participants into their diagnoses. The overlap section refers to patients who had all 3 modalities recorded.\n",
        "\\\n",
        "\\\n",
        "![stats.jpeg](https://drive.google.com/uc?export=view&id=1GarYGPtULrL5Ka5uiQdaR1vkIopIRt4O)"
      ],
      "metadata": {
        "id": "VDrOM-NQSGBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preprocessing**\n",
        "\n",
        "The CSV files and the VCF files utilized in the data processing must be obtained directly from ADNI (see Data Source section above) and thus cannot be included in this project's repository, or within the notebook.\n",
        "\\\n",
        "\\\n",
        "As a result of having such a large collection of multimodal data, we must implement a sufficient amount of preprocessing tasks. Preprocessing tasks differ based off which data type we are working with, therefore there will be separate preprocessing goals for the clinical, imaging, and genetic data. The end goal of the data preprocessing is to have a combined diagnosis dataset by patient ID, where we take diagnoses from images, clinical, and genetic data to create one ground truth diagnosis file.\n",
        "\\\n",
        "\\\n",
        "For imaging data, the main goal of preprocessing is to have the data split into training and testing pools. For genetic data preprocessing, VCF files are first obtained for ADNI. Then, the vcftools package is used to filter the files based on chosen criteria, and furthermore can filter based on Alzheimer's related genes. Finally, we reduce the number of features on this data.\n",
        "\\\n",
        "\\\n",
        "**The code for preprocessing the data is NOT included in this notebook, but rather each preprocessing stage's notebook is stored in our project's GitHub repo for reference. Each stage of preprocessing will be implemented in it's own separate notebook which are stored in our GitHub repository.** We replicated the code from the original repository, and then modified it to point to our data that we downloaded, and fixed any errors that came about.\n",
        "\\\n",
        "\\\n",
        "Please refer to the folders with the prefix \"preprocess_x/\" in our GitHub for all of our data preprocessing implementation code:\n",
        "https://github.com/MersimRizmani/MADDi-Replication"
      ],
      "metadata": {
        "id": "J4GjgStMSImN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original authors experimented with a variety of different combinations involving the building of a model. Particularly, the built and train both **unimodal** models and **multi-modal** models. The purpose of this was for them to demonstrate the superior performance of their multimodal framework (MADDi) compared to the single-modality models trained on only clinical, genetic, or imaging data.\n",
        "\\\n",
        "\\\n",
        "Below we show the code that the original authors used to built both the unimodal and multimodal models. Training code for these models has been omitted due to resource constraints. At the current stage of our project, we are still in the process of finalizing the replication of the original auther's model and training it.\n",
        "\n",
        "The training code is available in our project's GitHub repository under the \"/training\" directory.\n",
        "\n",
        "https://github.com/MersimRizmani/MADDi-Replication/tree/main/training\n",
        "\n",
        "(see Next Steps and Plans in the last section)"
      ],
      "metadata": {
        "id": "zr5g-2lCtqS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input,Dense,Dropout,MaxPooling2D, Flatten, Conv2D, BatchNormalization, MultiHeadAttention, concatenate\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "dF5RtJznIJbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_random_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "seeds = random.sample(range(1, 200), 5)"
      ],
      "metadata": {
        "id": "h_snu2r4IIHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clinical Unimodal Model**"
      ],
      "metadata": {
        "id": "0LQe3PI4PpXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clinical unimodal model architecture consists of a neural network with 3 fully connected layers.\n",
        "\n",
        "In terms of tuning the hyperparameters, they found that the ones that gave the best accuracy for the clinical unimodal model are:\n",
        "\n",
        "- **Learning rate**: 0.0001\n",
        "- **Batch size**: 32\n",
        "- **Number of layers**: 3\n",
        "- **Dropout value**: {0.2, 0.3, 0.5}\n",
        "- **Number of epochs:** 100\n",
        "\n",
        "The best performing clinical unimodal model has an accuracy of 80.59%. More evaluations metrics will be described in the **Results and Analysis** section.\n",
        "\n",
        "The below code demonstrates the design and architecture of the clinical unimodal neural network, and the output that follows highlights the details of the layers, activation functions, output shapes, and layer types:"
      ],
      "metadata": {
        "id": "rBVrKHUiW_iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in seeds:\n",
        "    reset_random_seeds(seed)\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(128, input_shape = (185,), activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(64, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(50, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(3, activation = \"softmax\"))\n",
        "\n",
        "model.compile(Adam(learning_rate = 0.0001), \"sparse_categorical_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "h0yXQ2x5HqUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Genetic Unimodal Model**"
      ],
      "metadata": {
        "id": "OrIWn5SlP3-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The genetic unimodal model architecture consists of a neural network with 3 fully connected layers.\n",
        "\n",
        "In terms of tuning the hyperparameters, they found that the ones that gave the best accuracy for the genetic unimodal model are:\n",
        "\n",
        "- **Learning rate**: 0.001\n",
        "- **Batch size**: 32\n",
        "- **Number of layers**: 3\n",
        "- **Dropout value**: {0.3, 0.5}\n",
        "- **Number of epochs:** 50\n",
        "\n",
        "The best performing genetic unimodal model has an accuracy of 77.78%. More evaluations metrics will be described in the **Results and Analysis** section.\n",
        "\n",
        "The below code demonstrates the design and architecture of the genetic unimodal neural network, and the output that follows highlights the details of the layers, activation functions, output shapes, and layer types:"
      ],
      "metadata": {
        "id": "STQuVY1inOP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in seeds:\n",
        "    reset_random_seeds(seed)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_shape = (15965,), activation = \"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation = \"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(32, activation = \"relu\"))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(32, activation = \"relu\"))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "    model.add(Dense(3, activation = \"softmax\"))\n",
        "\n",
        "model.compile(Adam(learning_rate = 0.001), \"sparse_categorical_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qia4m315IVBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imaging Unimodal Model**"
      ],
      "metadata": {
        "id": "GvbMokFeQJQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The imaging unimodal model architecture consists of a convolutional neural network with 3 convolutional layers.\n",
        "\n",
        "In terms of tuning the hyperparameters, they found that the ones that gave the best accuracy for the clinical unimodal model are:\n",
        "\n",
        "- **Learning rate**: 0.001\n",
        "- **Batch size**: 32\n",
        "- **Number of layers**: 3\n",
        "- **Dropout value**: {0.3, 0.5}\n",
        "- **Number of epochs:** 50\n",
        "\n",
        "The best performing imaging unimodal model has an accuracy of 92.23%. More evaluations metrics will be described in the **Results and Analysis** section.\n",
        "\n",
        "The below code demonstrates the design and architecture of the imaging unimodal neural network, and the output that follows highlights the details of the layers, activation functions, output shapes, and layer types:"
      ],
      "metadata": {
        "id": "qEYp8i-vnR8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in seeds:\n",
        "    reset_random_seeds(seed)\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(100, (3, 3),  activation='relu', input_shape=(72, 72, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv2D(50, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(3, activation = \"softmax\"))\n",
        "\n",
        "model.compile(Adam(learning_rate = 0.001), \"sparse_categorical_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ns_J1Hr9Ixb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multimodal Alzheimer's Disease Diagnosis Framework (MADDi)**"
      ],
      "metadata": {
        "id": "nhtjxeVJQNST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The multimodal model architecture consists of components derived from each of the unimodal models.\n",
        "\n",
        "In terms of tuning the hyperparameters, they found that the ones that gave the best accuracy for the clinical unimodal model are:\n",
        "\n",
        "- **Learning rate**: 0.001\n",
        "- **Batch size**: 32\n",
        "- **Number of layers**: {3, 3, 3}\n",
        "- **Dropout value**: {0.2, 0.3, 0.5}\n",
        "- **Number of epochs:** 50\n",
        "\n",
        "The best performing multimodal model has an average accuracy of 96.88% +/- 3.33%. More evaluations metrics will be described in the **Results and Analysis** section.\n",
        "\n",
        "The below code demonstrates the design and architecture of the multimodal model, and the output that follows highlights the details of the layers, activation functions, output shapes, and layer types:"
      ],
      "metadata": {
        "id": "CZdupUoGlVOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_img(t_img):\n",
        "    img = pd.read_pickle(t_img)\n",
        "    img_l = []\n",
        "    for i in range(len(img)):\n",
        "        img_l.append(img.values[i][0])\n",
        "\n",
        "    return np.array(img_l)"
      ],
      "metadata": {
        "id": "7cZjsmGyQsLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code block, there are modality-specific neural network architecture backbones developed in a single modality setting.\n",
        "\n",
        "As mentioned in the unimodal model discussions, those backbones consist of:\n",
        "- 3-layer fully connected neural network for clinical data\n",
        "- 3-layer fully connected neural network for genetic data\n",
        "- 3-layer convolutional neural network for imaging data"
      ],
      "metadata": {
        "id": "Wg7DHg21o3aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-layer fully connected neural network for genetic data\n",
        "def create_model_snp():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200,  activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(50, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    return model\n",
        "\n",
        "# 3-layer fully connected neural network for genetic data\n",
        "def create_model_clinical():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200,  activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(50, activation = \"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    return model\n",
        "\n",
        "# 3-layer convolutional neural network for imaging data\n",
        "def create_model_img():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(72, (3, 3), activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "1RTs20TSQwmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the above layers then enters a multi-headed self-attention layer, which allows the inputs to interact with each other and find what features should be paid most attention to within each modality. Then, that is followed by a cross-modal bidirectional layer, which does something similar, but across different pairs of modalities. The purpose of this is to identify and analyze interactions between different modalities, which in turn is the general purpose of building this multimodal framework and the purpose of this paper in the first place.\n",
        "\n",
        "The below code details the self-attention and cross-modal attention layers:"
      ],
      "metadata": {
        "id": "wc8uha6SprM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-modal attention layer\n",
        "def cross_modal_attention(x, y):\n",
        "\n",
        "    x = tf.expand_dims(x, axis=1)\n",
        "    y = tf.expand_dims(y, axis=1)\n",
        "    a1 = MultiHeadAttention(num_heads = 4,key_dim=50)(x, y)\n",
        "    a2 = MultiHeadAttention(num_heads = 4,key_dim=50)(y, x)\n",
        "    a1 = a1[:,0,:]\n",
        "    a2 = a2[:,0,:]\n",
        "    return concatenate([a1, a2])\n",
        "\n",
        "# Self-attention layer\n",
        "def self_attention(x):\n",
        "\n",
        "    x = tf.expand_dims(x, axis=1)\n",
        "    attention = MultiHeadAttention(num_heads = 4, key_dim=50)(x, x)\n",
        "    attention = attention[:,0,:]\n",
        "    return attention"
      ],
      "metadata": {
        "id": "-ZAiNeJaQ1Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is where we rebuilt the original author's MADDi framework. It utilizes the backbones from the single-modality models shown above via the create_model_clinical(), create_model_snp(), and create_model_img() methods, and demonstrates the ability to configure the attention mechanisms.\n",
        "\\\n",
        "\\\n",
        "The \"mode\" variable allows us to experiment with the aforementioned attention mechanism ablation. It allows us to compare results of toggling the presence of attention based on four criteria: self-attention and cross-modal attention, just cross-modal attention, and no attention. The variations will be generalized attention, self-attention, and cross-modal attention."
      ],
      "metadata": {
        "id": "pKiYnVFGqrJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_modal_model(mode, train_clinical, train_snp, train_img):\n",
        "\n",
        "    in_clinical = Input(shape=(train_clinical.shape[1]))\n",
        "\n",
        "    in_snp = Input(shape=(train_snp.shape[1]))\n",
        "\n",
        "    in_img = Input(shape=(train_img.shape[1], train_img.shape[2], train_img.shape[3]))\n",
        "\n",
        "    # Single-modality model backbones\n",
        "    dense_clinical = create_model_clinical()(in_clinical) # clinical unimodal model\n",
        "    dense_snp = create_model_snp()(in_snp) # genetic unimodal model\n",
        "    dense_img = create_model_img()(in_img) # imaging unimodal model\n",
        "\n",
        "    ########### Attention Layer ############\n",
        "\n",
        "    ## Cross Modal Bi-directional Attention ##\n",
        "\n",
        "    if mode == 'MM_BA':\n",
        "\n",
        "        vt_att = cross_modal_attention(dense_img, dense_clinical)\n",
        "        av_att = cross_modal_attention(dense_snp, dense_img)\n",
        "        ta_att = cross_modal_attention(dense_clinical, dense_snp)\n",
        "\n",
        "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
        "\n",
        "    ## Self Attention ##\n",
        "    elif mode == 'MM_SA':\n",
        "\n",
        "        vv_att = self_attention(dense_img)\n",
        "        tt_att = self_attention(dense_clinical)\n",
        "        aa_att = self_attention(dense_snp)\n",
        "\n",
        "        merged = concatenate([aa_att, vv_att, tt_att, dense_img, dense_snp, dense_clinical])\n",
        "\n",
        "    ## Self Attention and Cross Modal Bi-directional Attention##\n",
        "    elif mode == 'MM_SA_BA':\n",
        "\n",
        "        vv_att = self_attention(dense_img)\n",
        "        tt_att = self_attention(dense_clinical)\n",
        "        aa_att = self_attention(dense_snp)\n",
        "\n",
        "        vt_att = cross_modal_attention(vv_att, tt_att)\n",
        "        av_att = cross_modal_attention(aa_att, vv_att)\n",
        "        ta_att = cross_modal_attention(tt_att, aa_att)\n",
        "\n",
        "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
        "\n",
        "    ## No Attention ##\n",
        "    elif mode == 'None':\n",
        "\n",
        "        merged = concatenate([dense_img, dense_snp, dense_clinical])\n",
        "\n",
        "    else:\n",
        "        print (\"Mode must be one of 'MM_SA', 'MM_BA', 'MU_SA_BA' or 'None'.\")\n",
        "        return\n",
        "\n",
        "    ########### Output Layer ############\n",
        "\n",
        "    output = Dense(3, activation='softmax')(merged)\n",
        "    model = Model([in_clinical, in_snp, in_img], output)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "E5y4R9iZJVR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we are still in the process of preprocessing our data due to the large volume, and computation challenges, the training and test sets referenced in the code below are not yet available. That being said, all we need in terms of this notebook for designing the model, are the shapes of the datasets which we will have once data preprocessing is complete."
      ],
      "metadata": {
        "id": "iVFz159ftRtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model #\n",
        "\n",
        "### WE JUST NEED THE SHAPES OF THESE TRAINING AND TEST SETS ##################\n",
        "# train_clinical = pd.read_csv(\"X_train_clinical.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "# test_clinical= pd.read_csv(\"X_test_clinical.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "\n",
        "# train_snp = pd.read_csv(\"X_train_snp.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "# test_snp = pd.read_csv(\"X_test_snp.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "\n",
        "# train_img= make_img(\"X_train_img.pkl\")\n",
        "# test_img= make_img(\"X_test_img.pkl\")\n",
        "##############################################################################\n",
        "\n",
        "# model = multi_modal_model('MM_SA_BA', train_clinical, train_snp, train_img)\n",
        "# model.compile(optimizer=Adam(learning_rate = 0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "IKi-Ohk4Q4Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Training"
      ],
      "metadata": {
        "id": "MXAXOLXFwfAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the current stage of our project, we are still in the process of finalizing the replication of the original auther's model and training it.\n",
        "\n",
        "The training code is available in our project's GitHub repository under the \"/training\" directory.\n",
        "\n",
        "https://github.com/MersimRizmani/MADDi-Replication/tree/main/training\n",
        "\n",
        "(see Next Steps and Plans in the last section)"
      ],
      "metadata": {
        "id": "YLv2gXBh5xej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is how the unimodal models get trained:"
      ],
      "metadata": {
        "id": "CHhfNhta6LQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_pickle(\"X_train_c.pkl\")\n",
        "y_train = pd.read_pickle(\"y_train_c.pkl\")\n",
        "\n",
        "model.fit(X_train, y_train,  epochs=100, validation_split=0.1, batch_size=32,verbose=1)"
      ],
      "metadata": {
        "id": "Wttkis-G6HbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is how the multimodal model gets trained:"
      ],
      "metadata": {
        "id": "NZrz-cne6TVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "train_clinical = pd.read_csv(\"X_train_clinical.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "train_snp = pd.read_csv(\"X_train_snp.csv\").drop(\"Unnamed: 0\", axis=1).values\n",
        "train_img= make_img(\"X_train_img.pkl\")\n",
        "train_label= pd.read_csv(\"y_train.csv\").drop(\"Unnamed: 0\", axis=1).values.astype(\"int\").flatten()\n",
        "\n",
        "class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(train_label),y = train_label)\n",
        "d_class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "model.fit([train_clinical,\n",
        "            train_snp,\n",
        "            train_img],\n",
        "            train_label,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            class_weight=d_class_weights,\n",
        "            validation_split=0.1,\n",
        "            verbose=1)"
      ],
      "metadata": {
        "id": "eLk-xPEU6Yn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Evaluation"
      ],
      "metadata": {
        "id": "doLnq8SfwjVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the current stage of our project, we are still in the process of finalizing the replication of the original auther's model and training it, therefore we cannot provide an accurate evaluation of our model at this time.\n",
        "\n",
        "However, once we finalize the model and train it, we will collect the following evaluation metrics from the results:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1-Score\n",
        "\n",
        "According to the original authors, the F1-Score was the primary performance metric for evaluating the baselines, and accuracy was used to evaluate their best model against previous papers.\n",
        "\n",
        "(see Next Steps and Plans in the last section)\n",
        "\n",
        "The following will be the code used for the evaluation metrics for the unimodal models:"
      ],
      "metadata": {
        "id": "hTaZ7GsGvuee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "      # ... from within for for-loop for model training\n",
        "      acc = []\n",
        "      f1 = []\n",
        "      precision = []\n",
        "      recall = []\n",
        "\n",
        "      print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "      acc.append(score[1])\n",
        "\n",
        "      test_predictions = model.predict(X_test)\n",
        "      test_label = to_categorical(y_test,3)\n",
        "\n",
        "      true_label= np.argmax(test_label, axis =1)\n",
        "\n",
        "      predicted_label= np.argmax(test_predictions, axis =1)\n",
        "\n",
        "      cr = classification_report(true_label, predicted_label, output_dict=True)\n",
        "      precision.append(cr[\"macro avg\"][\"precision\"])\n",
        "      recall.append(cr[\"macro avg\"][\"recall\"])\n",
        "      f1.append(cr[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "print(\"Avg accuracy: \" + str(np.array(acc).mean()))\n",
        "print(\"Avg precision: \" + str(np.array(precision).mean()))\n",
        "print(\"Avg recall: \" + str(np.array(recall).mean()))\n",
        "print(\"Avg f1: \" + str(np.array(f1).mean()))\n",
        "print(\"Std accuracy: \" + str(np.array(acc).std()))\n",
        "print(\"Std precision: \" + str(np.array(precision).std()))\n",
        "print(\"Std recall: \" + str(np.array(recall).std()))\n",
        "print(\"Std f1: \" + str(np.array(f1).std()))\n",
        "print(acc)\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ],
      "metadata": {
        "id": "ZCb3hSdPvrBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following will be the code used for the evaluation metrics for the multimodal model:"
      ],
      "metadata": {
        "id": "8WH2Ddj_wiyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_classification_report(y_tru, y_prd, mode, learning_rate, batch_size,epochs, figsize=(7, 7), ax=None):\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
        "    yticks = [\"Control\", \"Moderate\", \"Alzheimer's\" ]\n",
        "    yticks += ['avg']\n",
        "\n",
        "    rep = np.array(precision_recall_fscore_support(y_tru, y_prd)).T\n",
        "    avg = np.mean(rep, axis=0)\n",
        "    avg[-1] = np.sum(rep[:, -1])\n",
        "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
        "\n",
        "    sns.heatmap(rep,\n",
        "                annot=True,\n",
        "                cbar=False,\n",
        "                xticklabels=xticks,\n",
        "                yticklabels=yticks,\n",
        "                ax=ax, cmap = \"Blues\")\n",
        "\n",
        "    plt.savefig('report_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'_' + str(epochs)+'.png')\n",
        "\n",
        "\n",
        "\n",
        "def calc_confusion_matrix(result, test_label,mode, learning_rate, batch_size, epochs):\n",
        "    test_label = to_categorical(test_label,3)\n",
        "\n",
        "    true_label= np.argmax(test_label, axis =1)\n",
        "\n",
        "    predicted_label= np.argmax(result, axis =1)\n",
        "\n",
        "    n_classes = 3\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    thres = dict()\n",
        "    for i in range(n_classes):\n",
        "        precision[i], recall[i], thres[i] = precision_recall_curve(test_label[:, i],\n",
        "                                                            result[:, i])\n",
        "\n",
        "\n",
        "    print (\"Classification Report :\")\n",
        "    print (classification_report(true_label, predicted_label))\n",
        "    cr = classification_report(true_label, predicted_label, output_dict=True)\n",
        "    return cr, precision, recall, thres"
      ],
      "metadata": {
        "id": "w_U94x5jwpwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate([test_clinical, test_snp, test_img], test_label)\n",
        "\n",
        "acc = score[1]\n",
        "test_predictions = model.predict([test_clinical, test_snp, test_img])\n",
        "cr, precision_d, recall_d, thres = calc_confusion_matrix(test_predictions, test_label, mode, learning_rate, batch_size, epochs)"
      ],
      "metadata": {
        "id": "BFvbKn6mw69D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Analysis\n",
        "\n",
        "At the current stage of our project, we are still in the process of finalizing the replication of the original auther's model and training it, therefore we cannot provide accurate results or analysis at this time.\n",
        "\n",
        "(see Next Steps and Plans in the last section)\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Paper Reproducibility**\n",
        "\n",
        "Based on our initial work so far, which includes replicating the data preprocessing stage, and an attempt at rebuilding the MADDi model, it seems as though this paper is in fact reproducible to some extent.\n",
        "\\\n",
        "\\\n",
        "We will have a clearer picturer on the reproducibility once we kickoff the training of the model."
      ],
      "metadata": {
        "id": "Bghxr3b1Siha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reproduction Eases**\n",
        "\n",
        "There were several aspects of the reproduction of the original paper that we found to be easy:\n",
        "\n",
        "- **Code access**: the code for the original paper was made pubicly available to us in a GitHub repository, and allowed for utiliziation without restriction.\n",
        "- **Research paper clarity**: the original research paper was very concise in it's explanations on the background of the problem, what they built, the hypothesis they were testing, and their results, which made it easier for us to attempt to replicate their implementation."
      ],
      "metadata": {
        "id": "2dFb9tIHSjiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reproduction Challenges**\n",
        "\n",
        "There were several challenges that we encountered while attempting to reproduce the results of this paper:\n",
        "\n",
        "- **Getting access to the data**: the process for requesting access to the ADNI data had a turnaround of around 2-3 weeks, which delayed our ability to begin work on this project.\n",
        "- **Volume of data**: once we were granted access to the ADNI data archive, we were overwhelmed with the volume of data that was available, spanning across several years. The original paper did not include and detail about where to obtain data from, from within the archive.\n",
        "- **Size of the data**: once we were able to identify the correct datasets within the archive, the process for transferring it over to our local machines/cloud storage took several days because of the amount of data and the large file sizes.\n",
        "- **Data preprocessing**: Without a clear indication of the environment the authors used, along with the computational capibilities of that environment, we didn't have any information on how to reproduce the data in an appropriate environment considering the amount of RAM and storage required. We're utilizing Google Colab and it's different processing features, but are also limited by the time limits on usage.\n",
        "- **Original code modifications**: the original code needed modifications to point to current data, as the code in the original repository was outdated, pointing to old data."
      ],
      "metadata": {
        "id": "3IBkQ05LSv89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Suggestions to the Authors**\n",
        "\n",
        "In an effort to make the reproducibility of this research more seamingless, the original authors should highlight the specific steps to preprocess the data such as:\n",
        "- Pinpointing which specific file names to download from ADNI: the pool of data available on the ADNI website is overwhelming large, containing data across several years, so letting the public know where to navigate to once they get access to the archive would expedite the process of reproducing this paper.\n",
        "- Include more data visualizations in the data preprocessing stage to help readers further understand the large datasets they are working with.\n",
        "- Keeping code up to date: we found ourselves having to tweak the original code due to the fact that it was out-of-date or utilizing old data."
      ],
      "metadata": {
        "id": "8s5vbyBRS15z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Next Steps and Plans**\n",
        "\n",
        "Given that this current snapshot of our notebook is a draft, there are several plans that we have to finishing up the replication of this project for the final draft:\n",
        "- Continue preprocessing the data. We are currently limited by colab's usage limitations and issues we are seeing with file downloads from ADNI, such as potentially corrupted vcf files for genetic data. We are presently working through these problems.\n",
        "- Finish rebuilding the multimodal deep learning model (MADDi): for the most we have the implementation of the original author's MADDi framework replicated, although there are some tweaks and modifications that we need to make to cater it to our current data.\n",
        "- Training the model: the bulk of the work between now and our final draft revolves around training the MADDi model. This includes experimenting with our ablations (i.e. Unified Hyperparameter Tuning Scheme and Attention Mechanisms).\n",
        "- Finally, we will evaluate our model and analyze the results."
      ],
      "metadata": {
        "id": "YDV_BHo_S2VN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Naqvi E. Alzheimer’s Disease Statistics. 2017. https://alzheimersnewstoday.com/alzheimers-disease-statistics/. Accessed June 20, 2022.\n",
        "\n",
        "Thies W, Bleiler L. 2013 Alzheimer’s Disease Facts and Figures. Wiley Online Library; 2013. https://alz-journals.onlinelibrary.wiley.com/doi/10.1016/j.jalz.2013.02.003. Accessed June 20, 2022.\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}